---
layout: default
tags: about
---
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
function readMore() {
    $('#readMore').hide();
    $('#more').show();
}
function readLess() {
    $('#readMore').show();
    $('#more').hide();
}
</script>

<img src="images/circle-cropped (6).png" alt="Vaishnavi Khindkar" width="240" style="float: right; padding: 20px; border-radius: 50%;" />

<div class="bio" style="text-align:justify">

<p>I'm a Research Fellow in computer vision and deep learning at the <a href="https://cvit.iiit.ac.in">Center for Visual Information Technology (CVIT)</a>, IIIT Hyderabad. I have been working in autonomous driving domain using computer vision and deep learning for a span of two years under the guidance of excellent team of advisors, <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C V Jawahar</a>, <a href="https://www.iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>, <a href="https://www.cse.iitd.ac.in/~chetan/">Prof. Chetan Arora</a> and <a href="https://www.intel.com/content/www/us/en/artificial-intelligence/bios/anbumani-subramanian.html">Dr. Anbumani Subramanian</a>. It has been a really great learning experience to grow in this field, that I am extremely passionate about. I worked on various amazing projects in this span of time. One of which has been published at WACV 2022 conference and other is under review at ECCV. One of our works is also under patenting process. I have also have been a reviewer at WACV conference. Overall it has really been a wonderful experience. Previously I had been working in IT sector at Barclays Inc., where I had good exposure to develop end-to-end systems and understand how things work at corporate level. It was a drastic career change (from Barclays to CVIT, IIITH), that I worked hard for; to pursue my passion as my profession and grow further in that path. I completed my Bachelors in Computer Science and Engineering in 2018 from Savitribai Phule Pune University (SPPU), India where I was advised by <a href="https://scholar.google.co.in/citations?user=IrfavGQAAAAJ&hl=en">Dr. Sudeep Thepade
</a>.
</p>
<br>

<p><strong>Research Interests: <span style="color:gray">Back during my undergraduate course in Computer Science, I used to wonder, "What really would be the next revolution in this field?" Back then enjoying my programming courses and being dramatically fascinated by technologies like Jarvis, Siri I thought, Mind Reading Computers would just be the next remarkable revolution. Curiosity about developing Mind-Reading Computers drove me towards emerging world-changing technologies in the field of Artificial Intelligence. I am fascinated by research works in object-Object interactions or Object-Scene interactions and their relationship / intent understanding. My current research work lies in the intersection of vision and language, using explainable AI for problems like pedestrian intent prediction and further exploring visual and abductive reasoning for the same. Some of my previous research works include interesting domains like unsupervised domain adaptation, spatio-temporal reasoning using graph neural networks, generative adversarial networks to name a few.</p></span></strong>

<p><strong>Volunteering: <span style="color:gray">I belive in giving back to the society what we are fortunate enough to have gained. I am selected to be a Portfolio Project Mentor for the Changemakers in AI program at AI4ALL. Excited to spend my summer mentoring the students at a platform like AI4ALL, it's a two-way learning process I believe. Previously I had also organised Organised HOUR OF CODE as an initiative for International Coding Week, to teach school students Coding and Algorithmic Concepts by innovative and simple games.</p></span></strong>

<p> My non-professional interests include watching Sci-Fi movies, Favourite being Iron Man. Reading a lot about new tech stuffs as well as trending articles and implementations related to my field of interest on Google and Twitter. Iâ€™m always up for interesting collaborations or just random chats on AI, feel free to drop me a message on Linkedin or via email.</p>

<br/>

<div class="container">
  <div class="row" style="text-align:center;">
        <div class="col">
          <a href="https://ai-4-all.org/"><img src="images/ai4all.png" style="max-height:150px;width:90%"></a>
        </div>
        <div class="col" style="text-align:center;">
          <a href="https://www.iiit.ac.in/"><img src="images/IIITH.png" style="max-height:500px;width:100%"></a>
        </div>
        <div class="col">
          <a href="https://cvit.iiit.ac.in/"><img src="images/cvit.png" style="width:80%;max-height:150px"></a>
        </div>      
        <div class="col" style="text-align:center;">
          <a href="https://home.barclays/"><img src="images/barclays.png" style="width:100%;max-height:500px"></a>
        </div>
        <div class="col">
          <a href="http://www.unipune.ac.in/"><img src="images/sppu.png" style="width:100%;max-height:150px"></a>
        </div>
  </div>
  <div class="row" style="text-align:center;">
        <div class="col">
          <div style="padding:10px"><h6>Summer 2022</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>Aug 2020-Current</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>Aug 2020-Current</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>2018-2020</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>2015-2018</h6></div>
        </div>
  </div>
</div>

<!-- <hr/> -->
<br/>

<div id="research">
<h2><a name="research">Research</a></h2>
<br/>

<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
      <tr>
      <td width="30%">
        <div class="one" style="text-align:center;">
            <img src="images/maindiagram.png" style="max-height: 400px;">
        </div>
      </td>
      <td valign="top" width="70%">
        <h5>
        Multi-Domain Incremental Learning for Semantic Segmentation
        </h5>
        <p class="authors">
        <b>Prachi Garg</b>, Rohit Saluja, Vineeth N Balasubramanian, Chetan Arora, Anbumani Subramanian, C V Jawahar
        </p>
        <p>
        IEEE Winter Conference on Applications of Computer Vision, WACV 2022
        </p>
        <p>
        <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Garg_Multi-Domain_Incremental_Learning_for_Semantic_Segmentation_WACV_2022_paper.pdf">Paper / </a>
        <!-- <i><a href="https://arxiv.org/abs/2110.12205">arxiv / </a></i> -->
        <a href="https://www.youtube.com/watch?v=YQC5KLZUpyc">Video / </a>
        <a href="https://github.com/prachigarg23/MDIL-SS">Code / </a>
        <a href="reports/294-wacv-poster.pdf">Poster / </a>
        <a href="https://openaccess.thecvf.com/content/WACV2022/supplemental/Garg_Multi-Domain_Incremental_Learning_WACV_2022_supplemental.pdf">Supplementary</a>
        </p>
        <!-- <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Garg_Multi-Domain_Incremental_Learning_for_Semantic_Segmentation_WACV_2022_paper.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">paper</a>
        <a href="https://arxiv.org/abs/2110.12205" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">arxiv</a>
        <a href="https://www.youtube.com/watch?v=YQC5KLZUpyc" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">video</a>
        <a href="https://github.com/prachigarg23/MDIL-SS" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">code</a>
        <a href="reports/294-wacv-poster.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">poster</a>
        <a href="https://openaccess.thecvf.com/content/WACV2022/supplemental/Garg_Multi-Domain_Incremental_Learning_WACV_2022_supplemental.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">supplementary</a> -->
      </td>
    </tr>
    <tr>
    <td width="30%">
      <div class="one" style="text-align:center;">
          <img src="images/diagram-ibm.png" style="max-height: 200px;">
      </div>
    </td>
    <td valign="top" width="70%">
      <h5>
      Towards an AI Infused System for Objectionable Content Detection in OTT
      </h5>
      <p class="authors">
      <b>Prachi Garg</b>, Shivang Chopra, Mudit Saxena, Anshu Yadav, Aditya Atri, Nishtha Madaan, Sameep Mehta
      </p>
      <p>
        <a href="https://www.linkedin.com/pulse/towards-ai-infused-system-personalization-video-content-madaan/?articleId=6699399407223218177">Blog-post / </a>
        <a href="https://drive.google.com/drive/folders/1DVUCCE7OLlA7nFbgsf3VanIH_pZRprrq?usp=sharing">Demo / </a>
        <a href="https://kidify-ibm.herokuapp.com/survey.html">Survey</a>
      </p>
      <p>In recent years, there has been a substantial increase in the consumption of OTT content. The ease of access and lack of regulatory checks in the online platforms have further led to a lot of differences in the content and the consuming audience. A lot of these scenarios might make majority of content unsuitable for family or children viewing. In this work, we propose a framework to address this problem by proposing filters which detect and highlight objectionable content in videos. We propose a framework which leverages multiple modalities like videos, subtitle text and audio to detect violence, explicit adult content, offensive speech in videos. </p>
      <!-- <a href="https://www.linkedin.com/pulse/towards-ai-infused-system-personalization-video-content-madaan/?articleId=6699399407223218177" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Blog-post</a>
      <a href="https://drive.google.com/drive/folders/1DVUCCE7OLlA7nFbgsf3VanIH_pZRprrq?usp=sharing" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Demo</a>
      <a href="https://kidify-ibm.herokuapp.com/survey.html" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Survey</a> -->
    </td>
  </tr>
  <tr>
  <td width="30%">
    <div class="one" style="text-align:center;">
        <img src="images/greyc-proposed-model.png" style="max-height: 200px;">
    </div>
  </td>
  <td valign="top" width="70%">
    <h5>
    Memorization and Generalization in CNNs using Soft Gating Mechanisms
    </h5>
    <p class="authors">
    <b>Prachi Garg</b>, Shivang Agarwal, Alexis Lechervy, Frederic Jurie
    </p>
    <p>
    <a href="reports/Report-GREYC.pdf">Report / </a>
    <a href="https://github.com/prachigarg23/Memorisation-and-Generalisation-in-Deep-CNNs-Using-Soft-Gating-Mechanisms">Code / </a>
    <a href="reports/report-failed-models-GREYC.pdf">Suboptimal ResNet Gating Mechanisms</a>
    </p>
    <p>A deep neural network learns patterns to hypothesize a large subset of samples that lie in-distribution and it memorises any out-of-distribution samples. While fitting to noise, the generalisation error increases and the DNN performs poorly on test set. In this work, we aim to examine if dedicating different layers to the generalizable and memorizable samples in a DNN could simplify the decision boundary learnt by the network and lead to improved generalization in DNNs. While the initial layers that are common to all examples tend to learn general patterns, we dedicate certain deeper additional layers in the network to memorise the out-of-distribution examples.</p>
    <!-- <a href="reports/Report-GREYC.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Report</a>
    <a href="https://github.com/prachigarg23/Memorisation-and-Generalisation-in-Deep-CNNs-Using-Soft-Gating-Mechanisms" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
    <a href="reports/report-failed-models-GREYC.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Suboptimal ResNet Gating Mechanisms</a> -->
  </td>
  </tr>
  <tr>
  <td width="30%">
    <div class="one" style="text-align:center;">
        <img src="images/major-archi.png" style="max-height: 200px;">
    </div>
  </td>
  <td valign="top" width="70%">
    <h5>
    ML-GAT: Multi-label Node Classification using Enhanced Graph Attention Network
    </h5>
    <p class="authors">
    <b>Prachi Garg*</b>, Ashi Gupta*, Rajni Jindal
    </p>
    <p>
      <!-- <a href="http://sersc.org/journals/index.php/IJAST/article/view/22330/11234">Paper / </a> -->
      <a href="reports/major-thesis-report.pdf">Thesis / </a>
      <a href="reports/MAJOR-PPT.pdf">Slides</a>
    </p>
    <p>
    Many real-world graph based problems require the assignment of more than one label to each node instance in the graph. We study here multi-label node classification using enhanced graph neural networks. We propose a novel architecture, Multi-Label Graph attention Network (ML-GAT) that leverages the applicability of the attention based Graph Attention Network (GAT) to efficient inductive semi-supervised multi-label classification by augmenting complex inter-label and node-label dependencies implicit in the graph structure to the learning process. Our model achieves 15.01% increase over the current state-of-the-art ML-GCN framework for Facebook dataset and 6% increase for the Yeast dataset. We analyse the influence of dropout and training size; and infer the relative importance of node-label and label-label dependencies.
    </p>
    <!-- <a href="http://sersc.org/journals/index.php/IJAST/article/view/22330/11234" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
    <a href="reports/major-thesis-report.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Thesis</a>
    <a href="reports/MAJOR-PPT.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a> -->
    <p>
      *Denotes equal contribution
    </p>
  </td>
  </tr>
  <tr>
  <td width="30%">
    <div class="one" style="text-align:center;">
        <img src="images/terra-icognita.png" style="max-height: 200px;">
    </div>
  </td>
  <td valign="top" width="70%">
    <h5>
    Visual Wildlife Monitoring: Domain Generalization for Animal Detection in the Wild
    </h5>
    <p class="authors">
    <b>Prachi Garg</b>, Gullal Cheema, Saket Anand
    </p>
    <p>
    I worked towards benchmarking species detection in camera trap images from unconstrained wild environments to generalise to new environments using state-of-the-art Faster-RCNN variants. The <a href="https://beerys.github.io/CaltechCameraTraps/">Catech Camera Traps dataset (CCT20)</a> is an unconstrained wild environment camera traps dataset designed to study domain generalization for animal species. It contains test data collected from both, locations that are same as train data (cis) as well as locations different from train data (trans). Factors like illumination, motion blur, occlusion, camouflage and perspective can severely affect the performance of species recognition systems. We bridged the generalization gap between cis-locations (test domain same as train) and trans-locations (unseen test domain) performance from 26.8% to 22.9% by using state-of-the-art Faster-RCNN variants.
    </p>
    <!-- While visual analytics plays an essential role in wildlife monitoring, camera trap images from unconstrained wild environments pose several challenges for species detection and classification.  Most existing models show poor generalisation on the test set domains  due to drastic domain shifts, transfer learning is difficult. We conducted experiments with modified versions of state-of-the-art object detection frameworks, incorporating concepts such as receptive field blocks from RBFNet, feature pyramid pooling from FPN, and DA-FRCNN aimed at training a model that learns a richer feature representationÂ so as to bridge the gap between cis and trans performance for animal detection in the <a href="https://beerys.github.io/CaltechCameraTraps/">Catech Camera Traps dataset (CCT20)</a>. -->
    <p><small>More details about my research journey can be found <a href="reports/Research_Journey.pdf">here</a>.</small></p>
  </td>
  </tr>
  </tbody>
</table>
</div>

<br/>


<div id="talks">
<h2>News and Miscellaneous</h2>
<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
      <tr>
      <td width="20%">
        <h5>
          January, 2022
        </h5>
      </td>
      <td valign="middle" width="80%">
        <h5>
        Presented our work on 'Multi-domain incremental learning for semantic segmentation' at WACV 2022
        </h5>
      </td>
      </tr>
      <tr>
      <td width="20%">
        <h5>
          December, 2021
        </h5>
      </td>
      <td valign="middle" width="80%">
        <h5>
        Funded to be a Super Volunteer, WiML Workshop @ NeurIPS 2021; Attended NeurIPS 2021 Workshop on Machine Learning for Autonomous Driving
        </h5>
      </td>
      </tr>
      <tr>
      <td width="20%">
        <h5>
          August, 2021
        </h5>
      </td>
      <td valign="middle" width="80%">
        <h5>
        Sub Reviewer, BMVC 2021
        </h5>
      </td>
      </tr>
      <tr>
      <td width="20%">
        <h5>
          September, 2020
        </h5>
      </td>
      <td valign="middle" width="80%">
        <h5>
        Gave a tutorial on Geometric Deep Learning and Graph Convolutional Networks (GCN)
        </h5>
        <p class="authors">
        Reading Group: Vision for Mobility and Safety
        <br>
        <a href="reports/GCN-Tutorial-Wed-meeting.pdf">Slides</a>
        </p>
      </td>
      </tr>
  </tbody>
</table>
</div>
<hr>
<p align="right">
<small>Forked and modified from <a href="https://virajprabhu.github.io/">Viraj Prabhu's</a> adaptation of <a href="https://github.com/johno/pixyll">Pixyll</a> theme</a></small></p>
